%%%%%%%% ICML 2019 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

\usepackage{kotex}
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2019} with \usepackage[nohyperref]{icml2019} above.
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2019}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2019}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{COSE474-2019F: Final Project Report}

\begin{document}
	
	\twocolumn[
	\icmltitle{COSE474-2019F: Final Project Report \\
		Improving Detecting DeepFake Videos with Fake Real Dataset}
	
	% It is OKAY to include author information, even for blind
	% submissions: the style file will automatically remove it for you
	% unless you've provided the [accepted] option to the icml2019
	% package.
	
	% List of affiliations: The first argument should be a (short)
	% identifier you will use later to specify author affiliations
	% Academic affiliations should list Department, University, City, Region, Country
	% Industry affiliations should list Company, City, Region, Country
	
	% You can specify symbols, otherwise they are numbered in order.
	% Ideally, you should not use this facility. Affiliations will be numbered
	% in order of appearance and this is the preferred way.
	\icmlsetsymbol{equal}{*}
	
	\begin{icmlauthorlist}
		\icmlauthor{제한재}{}
		\icmlauthor{허환}{}
		\icmlauthor{홍주연}{}
		\icmlauthor{도재형}{}
		\icmlauthor{채병주}{}
	\end{icmlauthorlist}
	
	%\icmlaffiliation{ku}{Department of Computer Science \& Engineering, Korea University, Seoul, Korea}
	
	
	%\icmlcorrespondingauthor{the}{myemail@korea.ac.kr}
	%\icmlcorrespondingauthor{Eee Pppp}{ep@eden.co.uk}
	
	% You may provide any keywords that you
	% find helpful for describing your paper; these are used to populate
	% the "keywords" metadata in the PDF but will not be shown in the document
	\icmlkeywords{Machine Learning, ICML}
	
	\vskip 0.3in
	]
	
	% this must go after the closing bracket ] following \twocolumn[ ...
	
	% This command actually creates the footnote in the first column
	% listing the affiliations and the copyright notice.
	% The command takes one argument, which is text to display at the start of the footnote.
	% The \icmlEqualContribution command is standard text for equal contribution.
	% Remove it (just {}) if you do not need this facility.
	
	%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
	%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
	
	\begin{abstract}
		이 프로젝트는 주어진 input video가 original video인지 딥러닝 모델로 생성된 manipulated DeepFake video (이하 DeepFake)인지 detect하는 model을 구현하고, 이 model의 성능을 향상시키기 위한 다양한 방법에 대해 탐구하였다. FaceForensics++ \cite{roessler2019faceforensicspp} 및 Google에서 DeepFake 생성용으로 촬영한 영상과 youtube 영상을 original 영상으로 채택하고 DeepFake, Face2Face, FaceSwap, NeuralTextures 방식으로 생성한 영상과 전용 DeepFakeDetection을 dataset으로 사용하였다.
	\end{abstract}
	
	\section{Introduction}
	% Motivation & Problem definition
	DeepFake는 GAN과 같은 deep neural network를 사용하여 사람의 얼굴을 합성하는 기술이다. 일상적인 사진들로도 쉽고 빠르게 자연스러운 결과물을 생성할 수 있고, 심지어 동영상으로 촬영된 영상도 자연스럽게 합성한다. 하지만, DeepFake 기술의 발전과 social media 상에서 많은 data를 구할 수 있게 되면서 타인을 사칭하는 계정과 FakeVideo로 기업의 Security를 위협하거나 DeepFake pornography등 사람이 쉽게 판별하기 어렵다는 점을 악용하는 경우 사회적으로 큰 문제가 되고 있다.
	
	이러한 문제들로 인해 DeepFake detection에 대한 연구의 필요성이 대두되었고, DeepFake detection은 현재 computer vision 분야의 주요 issue 중 하나가 되었다. 이에 Google에서는 DeepFake detection 연구를 위한 dataset을 공개하였고 \cite{googleblog}, Facebook에서도 DeepFake detection challenge를 dataset 공개과 함께 진행할 예정임을 알렸다 \cite{facebookblog}.
	
	따라서 이 프로젝트는 주어진 image/video가 DeepFake 를 이용해 합성되었는지 아닌지를 판단하는 DeepFake detection model을 개발하고, 그 성능을 평가해 보고자 한다.
	
	\section{Project Environment}
	
	처음 진행할때에는 Google Colab의 자원을 사용하여 프로젝트를 진행하였다. 하지만 대용량의 video dataset을 다운로드하는 과정에서 Colab의 유효 세션 길이가 12시간으로 한정되어 있어서 다른 환경의 필요성이 요구되었다. 이 프로젝트에서는 Google Colab 대신 GPU 서버를 호스팅하여 프로젝트를 진행하였다. GPU 서버에서는 Anaconda를 통해 Python 환경설정을 하고, Jupyter Notebook을 웹으로 접속하여 작업하는 방식으로 Colab에서 진행하던 방식을 유지하며 진행하였다.
	
	\section{Team Members}
	% Student id, name, role
	프로젝트의 모든 팀원이 모델 기획 및 구현에 참여하였다. \newline 
	제한재 (2014210110) -- Data preprocessing, Hosting Server Management\newline
	허환 (2015410124), 채병주 (2016320168) -- Model fine-tuning\newline
	홍주연 (2015410041), 도재형 (2016320210) -- SOTA model \& dataset research 
	\section{Related Works}
	
	현재 FaceForensics++ \cite{roessler2019faceforensicspp} 에서 제시하고 있는 SOTA라고 볼 수 있는 연구는 MesoNet\cite{darius2018MesoNet} 과 XceptionNet\cite{chollet2017xception}를 소개할 수 있다. 
	
		\subsection{DeepFake Datasets}
		% FaceForensics, google DFD
		DeepFake detection model의 학습을 위한 다양한 dataset들이 공개되어 있다. R\"ossler el al.은 fake video detection을 위한 FaceForensics++ dataset을 공개했다 \cite{roessler2019faceforensicspp}. 이 dataset은 DeepFakes 외에도 FaceSwap, NeuralTextures 등과 같은 다양한 방식의 video manipulation을 통해 만든 fake video들과, YouTube에 업로드 된 real video들을 모아 둔 dataset이다. 또, Google에서는 DeepFake Detection Dataset \cite{DDD_GoogleJigSaw2019} 을 공개하였는데, 이 dataset은 방대한 양의 DeepFake video data들을 포함하고 있다. 이 dataset 역시 FaceForensics++의 GitHub 
		repository에 함께 공개되어 있다.
		
		\subsection{Normal Image and Video Classifier}
		% Xception, ECO
		Xception 모델은 inception module을 이용하여 채널 간의 관계(Cross-Channel Correlation)을 찾아 분리하는 것과 spatial correlation을 분리하는 것을 목표로 합니다. 이 목표를 수행하기 위해서 모델은 1x1 Conv 로 Channel reduction 을 수행하는데, 이는 cross channel correlation을 매핑을 하고, output channel 을 spatial correlation 을 
		
		\subsection{DeepFake Video Detection Models}
		% Mesonet, Eye blinking
		압축된 비디오에서는 노이즈가 많기 때문에 frame 단위의 노이즈를 분석하는 것이 실질적으로 불가능하다. 반대로, 사람의 얼굴을 묘사할 때 거시적으로는 조작된 영상과 실제 영상을 구분하는 데에 어려움을 겪는다. 따라서 MesoNet은 그 이름에서 알 수 있듯이 mesoscopic 레벨의 분석을 통해서 DeepFake 영상을 구분한다. MesoNet은 test dataset에 따라 Meso-4 네트워크와 MesoInception-4 네트워크 2개의 네트워크로 이루어져 있다. 우선, Meso-4 네트워크의 경우 4개의 연속된 convolutional Network과 2개의 FC Net으로 이루어져 있다. MesoInception-4 네트워크의 경우 Meso-4 네트워크와 많이 유사하지만 앞의 2개의 Convolutional layer를 Inception 모듈로 대체했다는 차이점이 있다.
	
	\section{Architecture and Implementation or Something}
	% Baseline
	% Algorithm
	
	\section{Main Challenges}
	베이스라인 모델로 구현한 MesoNet 과 XceptionNet 은 sequential data를 다루지 않기 때문에 동영상 데이터가 가지는 'temporal feature'를 모델에서 표현할 수 없다. 두 모델의 구현은 개별 동영상 프레임을 잘라내어 각각의 channel 로서 frame corpus를 다루고 있다. 즉, 두 모델은 단순하게 각각의 frame이 DeepFake 로 생성된 frame 로 판별될 확률을 구하여, 전체 동영상의 probability 를 계산하고 있다. 여러 연구들은 개별 frame에서만 가지고 있는 정보 뿐만 아니라 전체 sequence 에서 유의미한 자료 (eg. Eye Blinking)를 활용해야 한다고 지적하고 있다. sequential data 를 활용하지 않는 모델은 성능을 확보하기 어렵다.
	
	\subsection{Implementation}
	
	
	
	\section{Conclusion}
	% Comparison with SOTA
	
	\section{Future Work}
	FaceForensics++ 에서 제공하는 데이터 중에는 얼굴 부분만 인식해서 표시해놓은 mask 영상이 있는데, DeepFake video는 얼굴 영역에 한해서만 원본을 편집하기 때문에, mask를 사용한다면 기대 성능을 더 높일 수 있을 것이다. 
	
	youtube 데이터 셋에서 overfit 되는 경향이 있어서, 다른 수치들보다 조금 더 false positive 판정 비율이 높았는데, 이 부분을 해결할 dataset 이 보강이 되거나 개선안을 찾아준다면 성능을 높일 수 있는 부분이 존재한다. 
	
	3D Tensor로 input을 처리하는 과정에서 training과 test 과정에서 굉장히 속도가 느렸다. 3D convolutional filter 에서 처리하는 dimension의 양이 너무 커지는데, 이러한 문제에 대해 적절한 해결 방법을 찾아야 한다.
	
	% In the unusual situation where you want a paper to appear in the
	% references without citing it in the main text, use \nocite
	
	
	\bibliography{report}
	\bibliographystyle{icml2019}
	
	
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019. Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
